train_csv_path: '/home/huutien/5_folds_split_3D/fold_1_train.csv'
val_csv_path: '/home/huutien/5_folds_split_3D/fold_1_val.csv'
data_dir: '/home/huutien/filter_ds'


project_name: "LBM"
run_name : "3D_training_LBM" 
save_dir : "/home/huutien/sources/GenerativeModels/3D_training/results"

pretrained_unet_path: "/home/huutien/sources/GenerativeModels/large_files/diffusion_model.pth"
vae_path: "/home/huutien/sources/GenerativeModels/3D_training/results/3D_training_KL_20251229_101810/best_checkpoint.pth"

n_epochs : 100
batch_size : 2
accumulate_grad_batches: 4 # tuning 
num_workers : 4
cache_rate : 1.0
use_bfloat16 : True
validation_interval: 1
# resume_state_path: "/home/huutien/sources/GenerativeModels/3D_training/results/3D_training_KL_20251229_101810/best_checkpoint.pth"

optimizer: 
  beta1: 0.9
  lr: 0.0001
  optimizer: Adam
  weight_decay: 0.0
lr_scheduler:
  factor: 0.5
  patience: 300
  threshold: 0.0001
  cooldown: 300
  min_lr: 5.e-7


EMA: 
  ema_decay: 0.995
  start_ema_step: 1500
  update_ema_interval: 8
  use_ema: True

lbm: 
  latent_loss_type: "l2"
  latent_loss_weight: 1.0
  pixel_loss_type: "l2"
  pixel_loss_weight: 10.0 # tuning 
  timestep_sampling: "custom_timesteps" # "log_normal", "custom_timesteps" tuning 
  logit_mean: 0.0
  logit_std: 1.0
  selected_timesteps: [250, 500, 750, 1000] # only used if timestep_sampling is "custom_timesteps"
  prob: [0.25, 0.25, 0.25, 0.25] # only used if timestep_sampling is "custom_timesteps"
  bridge_noise_sigma: 0.005 # tuning 