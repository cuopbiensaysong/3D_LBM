{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e2b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# CUDA VISIBLE DEVICE \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de64bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/envs/m3d/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.0\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.2.1+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
      "MONAI __file__: /home/anaconda3/envs/m3d/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scipy version: 1.13.0\n",
      "Pillow version: 10.3.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.17.1+cu121\n",
      "tqdm version: 4.66.2\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 7.0.0\n",
      "pandas version: 2.2.2\n",
      "einops version: 0.8.0\n",
      "transformers version: 4.39.1\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai import transforms\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.utils import first, set_determinism\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import L1Loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.inferers import LatentDiffusionInferer\n",
    "from generative.losses import PatchAdversarialLoss, PerceptualLoss\n",
    "from generative.networks.nets import AutoencoderKL, DiffusionModelUNet, PatchDiscriminator\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4277326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List, Literal, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class StateDictAdapter:\n",
    "    \"\"\"\n",
    "    StateDictAdapter for adapting the state dict of a model to a checkpoint state dict.\n",
    "\n",
    "    This class will iterate over all keys in the checkpoint state dict and filter them by a list of regex keys.\n",
    "    For each matching key, the class will adapt the checkpoint state dict to the model state dict.\n",
    "    Depending on the target size, the class will add missing blocks or cut the block.\n",
    "    When adding missing blocks, the class will use a strategy to fill the missing blocks: either adding zeros or normal random values.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```\n",
    "    adapter = StateDictAdapter()\n",
    "    new_state_dict = adapter(\n",
    "        model_state_dict=model.state_dict(),\n",
    "        checkpoint_state_dict=state_dict,\n",
    "        regex_keys=[\n",
    "            r\"class_embedding.linear_1.weight\",\n",
    "            r\"conv_in.weight\",\n",
    "            r\"(down_blocks|up_blocks)\\.\\d+\\.attentions\\.\\d+\\.transformer_blocks\\.\\d+\\.attn\\d+\\.(to_k|to_v)\\.weight\",\n",
    "            r\"mid_block\\.attentions\\.\\d+\\.transformer_blocks\\.\\d+\\.attn\\d+\\.(to_k|to_v)\\.weight\"\n",
    "        ]\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        model_state_dict (Dict[str, torch.Tensor]): The model state dict.\n",
    "        checkpoint_state_dict (Dict[str, torch.Tensor]): The checkpoint state dict.\n",
    "        regex_keys (Optional[List[str]]): A list of regex keys to adapt the checkpoint state dict. Defaults to None.\n",
    "            Passing a list of regex will drastically reduce the latency.\n",
    "            If None, all keys in the checkpoint state dict will be adapted.\n",
    "        strategy (Literal[\"zeros\", \"normal\"], optional): The strategy to fill the missing blocks. Defaults to \"normal\".\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _create_block(\n",
    "        self,\n",
    "        shape: List[int],\n",
    "        strategy: Literal[\"zeros\", \"normal\"],\n",
    "        input: torch.Tensor = None,\n",
    "    ):\n",
    "        if strategy == \"zeros\":\n",
    "            return torch.zeros(shape)\n",
    "        elif strategy == \"normal\":\n",
    "            if input is not None:\n",
    "                mean = input.mean().item()\n",
    "                std = input.std().item()\n",
    "                return torch.randn(shape) * std + mean\n",
    "            else:\n",
    "                return torch.randn(shape)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy {strategy}\")\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        model_state_dict: Dict[str, torch.Tensor],\n",
    "        checkpoint_state_dict: Dict[str, torch.Tensor],\n",
    "        regex_keys: Optional[List[str]] = None,\n",
    "        strategy: Literal[\"zeros\", \"normal\"] = \"normal\",\n",
    "    ):\n",
    "        start = time.perf_counter()\n",
    "        # if no regex keys are provided, we use all keys in the model state dict\n",
    "        if regex_keys is None:\n",
    "            regex_keys = list(model_state_dict.keys())\n",
    "\n",
    "        # iterate over all keys in the checkpoint state dict\n",
    "        for checkpoint_key in list(checkpoint_state_dict.keys()):\n",
    "            # iterate over all regex keys\n",
    "            for regex_key in regex_keys:\n",
    "                if re.match(regex_key, checkpoint_key):\n",
    "                    dst_shape = model_state_dict[checkpoint_key].shape\n",
    "                    src_shape = checkpoint_state_dict[checkpoint_key].shape\n",
    "\n",
    "                    ## Sizes adapter\n",
    "                    # if length of shapes are different, we need to unsqueeze or squeeze the tensor\n",
    "                    if len(dst_shape) != len(src_shape):\n",
    "                        # in the case [a] vs [a, b] -> unsqueeze [a, 1]\n",
    "                        if len(src_shape) == 1:\n",
    "                            checkpoint_state_dict[checkpoint_key] = (\n",
    "                                checkpoint_state_dict[checkpoint_key].unsqueeze(1)\n",
    "                            )\n",
    "                            logging.info(\n",
    "                                f\"Unsqueeze {checkpoint_key}: {src_shape} -> {checkpoint_state_dict[checkpoint_key].shape}\"\n",
    "                            )\n",
    "                        # in the case [a, b] vs [a] -> squeeze [a]\n",
    "                        elif len(dst_shape) == 1:\n",
    "                            checkpoint_state_dict[checkpoint_key] = (\n",
    "                                checkpoint_state_dict[checkpoint_key][:, 0]\n",
    "                            )\n",
    "                            logging.info(\n",
    "                                f\"Squeeze {checkpoint_key}: {src_shape} -> {checkpoint_state_dict[checkpoint_key].shape}\"\n",
    "                            )\n",
    "                        # in the other cases, raise an error\n",
    "                        else:\n",
    "                            raise ValueError(\n",
    "                                f\"Shapes of {checkpoint_key} are different: {dst_shape} != {src_shape}\"\n",
    "                            )\n",
    "\n",
    "                        # update the shapes\n",
    "                        dst_shape = model_state_dict[checkpoint_key].shape\n",
    "                        src_shape = checkpoint_state_dict[checkpoint_key].shape\n",
    "                        assert len(dst_shape) == len(\n",
    "                            src_shape\n",
    "                        ), f\"Shapes of {checkpoint_key} are different: {dst_shape} != {src_shape}\"\n",
    "\n",
    "                    ## Shapes adapter\n",
    "                    # modify the checkpoint state dict only if the shapes are different\n",
    "                    if dst_shape != src_shape:\n",
    "                        # create a copy of the tensor\n",
    "                        tmp = torch.clone(checkpoint_state_dict[checkpoint_key])\n",
    "\n",
    "                        # iterate over all dimensions\n",
    "                        for i in range(len(dst_shape)):\n",
    "                            if dst_shape[i] != src_shape[i]:\n",
    "                                diff = dst_shape[i] - src_shape[i]\n",
    "\n",
    "                                # if the difference is greater than 0, we need to add missing blocks\n",
    "                                if diff > 0:\n",
    "                                    missing_shape = list(tmp.shape)\n",
    "                                    missing_shape[i] = diff\n",
    "                                    missing = self._create_block(\n",
    "                                        shape=missing_shape,\n",
    "                                        strategy=strategy,\n",
    "                                        input=tmp,\n",
    "                                    )\n",
    "                                    tmp = torch.cat((tmp, missing), dim=i)\n",
    "                                    logging.info(\n",
    "                                        f\"Adapting {checkpoint_key} with strategy:{strategy} from shape {src_shape} to {dst_shape}\"\n",
    "                                    )\n",
    "                                # if the difference is less than 0, we need to cut the block\n",
    "                                else:\n",
    "                                    tmp = tmp.narrow(i, 0, dst_shape[i])\n",
    "                                    logging.info(\n",
    "                                        f\"Adapting {checkpoint_key} by narrowing from shape {src_shape} to {dst_shape}\"\n",
    "                                    )\n",
    "\n",
    "                        checkpoint_state_dict[checkpoint_key] = tmp\n",
    "        end = time.perf_counter()\n",
    "        logging.info(f\"StateDictAdapter took {end-start:.2f} seconds\")\n",
    "        return checkpoint_state_dict\n",
    "\n",
    "\n",
    "class StateDictRenamer:\n",
    "    \"\"\"\n",
    "    StateDictRenamer for renaming keys in a checkpoint state dict.\n",
    "    This class will iterate over all keys in the checkpoint state dict and rename them according to a rename dict.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        ```\n",
    "        renamer = StateDictRenamer()\n",
    "        new_state_dict = renamer(\n",
    "            checkpoint_state_dict=state_dict,\n",
    "            rename_dict={\n",
    "                \"add_embedding.linear_1.weight\": \"class_embedding.linear_1.weight\",\n",
    "                \"add_embedding.linear_1.bias\": \"class_embedding.linear_1.bias\",\n",
    "                \"add_embedding.linear_2.weight\": \"class_embedding.linear_2.weight\",\n",
    "                \"add_embedding.linear_2.bias\": \"class_embedding.linear_2.bias\",\n",
    "            }\n",
    "        )\n",
    "        ```\n",
    "\n",
    "    Args:\n",
    "\n",
    "        checkpoint_state_dict (Dict[str, torch.Tensor]): The checkpoint state dict.\n",
    "        rename_dict (Dict[str, str]): The dictionary mapping the old keys to new keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        checkpoint_state_dict: Dict[str, torch.Tensor],\n",
    "        rename_dict: Dict[str, str],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        for old_key, new_key in rename_dict.items():\n",
    "            if old_key not in checkpoint_state_dict:\n",
    "                logging.warning(f\"Key {old_key} not found in checkpoint state dict\")\n",
    "                continue\n",
    "            else:\n",
    "                assert (\n",
    "                    new_key not in checkpoint_state_dict\n",
    "                ), f\"Key {new_key} already exists in checkpoint state dict\"\n",
    "                checkpoint_state_dict[new_key] = checkpoint_state_dict.pop(old_key)\n",
    "                logging.info(f\"Renaming {old_key} to {new_key}\")\n",
    "        return checkpoint_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2b20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ea6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 \n",
    "def get_autoencoder_model_from_ckpt(ckpt_path):\n",
    "    # load state dict from ckpt\n",
    "    state_dict = torch.load(ckpt_path)\n",
    "    autoencoder_state_dict = state_dict[\"autoencoder_state\"]\n",
    "    autoencoder = AutoencoderKL(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        num_channels=(64, 128, 128, 128),\n",
    "        latent_channels=3,\n",
    "        num_res_blocks=2,\n",
    "        norm_num_groups=32,\n",
    "        norm_eps=1e-6,\n",
    "        attention_levels=(False, False, False, False),\n",
    "        with_encoder_nonlocal_attn=False,\n",
    "        with_decoder_nonlocal_attn=False,\n",
    "    )\n",
    "\n",
    "    autoencoder.load_state_dict(autoencoder_state_dict, strict=True)\n",
    "    return autoencoder\n",
    "\n",
    "autoencoder = get_autoencoder_model_from_ckpt('/home/huutien/sources/GenerativeModels/3D_training/results/3D_training_KL_20251229_101810/best_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77c21a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(2, 1, 160, 160, 96).to(device)\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# forward pass\n",
    "# with torch.no_grad():\n",
    "#     latent = autoencoder.encode_stage_2_inputs(inputs)\n",
    "\n",
    "# latent.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef055afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/envs/m3d/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/anaconda3/envs/m3d/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "discriminator = PatchDiscriminator(spatial_dims=3, num_layers_d=3, num_channels=32, in_channels=1, out_channels=1)\n",
    "discriminator.to(device)\n",
    "adv_loss = PatchAdversarialLoss(criterion=\"least_squares\")\n",
    "loss_perceptual = PerceptualLoss(spatial_dims=3, network_type=\"squeeze\", is_fake_3d=True, fake_3d_ratio=0.2)\n",
    "loss_perceptual.to(device)\n",
    "\n",
    "\n",
    "def KL_loss(z_mu, z_sigma):\n",
    "    kl_loss = 0.5 * torch.sum(z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1, dim=[1, 2, 3, 4])\n",
    "    return torch.sum(kl_loss) / kl_loss.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d59989f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction, z_mu, z_sigma = autoencoder(inputs)\n",
    "# kl_loss = KL_loss(z_mu, z_sigma)\n",
    "\n",
    "# logits_fake = discriminator(reconstruction.contiguous().detach())[-1]\n",
    "# loss_d_fake = adv_loss(logits_fake, target_is_real=False, for_discriminator=True)\n",
    "# logits_real = discriminator(inputs.contiguous().detach())[-1]\n",
    "# loss_d_real = adv_loss(logits_real, target_is_real=True, for_discriminator=True)\n",
    "# discriminator_loss = (loss_d_fake + loss_d_real) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0d07123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 20, 20, 12])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent = autoencoder.encode_stage_2_inputs(inputs)\n",
    "latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc28969b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 18, 18, 10]),\n",
       " torch.Size([2, 1, 160, 160, 96]),\n",
       " torch.Size([2, 3, 20, 20, 12]),\n",
       " torch.Size([2, 3, 20, 20, 12]),\n",
       " tensor(0.6692, grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_fake.shape, reconstruction.shape, z_mu.shape, z_sigma.shape, discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "297d3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model = DiffusionModelUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    num_channels=(256, 512, 768),\n",
    "    num_res_blocks=2,\n",
    "    attention_levels=(False, True, True),\n",
    "    norm_num_groups=32,\n",
    "    norm_eps=1e-6,\n",
    "    resblock_updown=True,\n",
    "    num_head_channels=[0, 512, 768],\n",
    "    with_conditioning=True,\n",
    "    transformer_num_layers=1,\n",
    "    cross_attention_dim=4,\n",
    "    upcast_attention=True,\n",
    "    use_flash_attention=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6358893a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv_in.conv.weight', 'conv_in.conv.bias', 'time_embed.0.weight', 'time_embed.0.bias', 'time_embed.2.weight', 'time_embed.2.bias', 'down_blocks.0.resnets.0.norm1.weight', 'down_blocks.0.resnets.0.norm1.bias', 'down_blocks.0.resnets.0.conv1.conv.weight', 'down_blocks.0.resnets.0.conv1.conv.bias', 'down_blocks.0.resnets.0.time_emb_proj.weight', 'down_blocks.0.resnets.0.time_emb_proj.bias', 'down_blocks.0.resnets.0.norm2.weight', 'down_blocks.0.resnets.0.norm2.bias', 'down_blocks.0.resnets.0.conv2.conv.weight', 'down_blocks.0.resnets.0.conv2.conv.bias', 'down_blocks.0.resnets.1.norm1.weight', 'down_blocks.0.resnets.1.norm1.bias', 'down_blocks.0.resnets.1.conv1.conv.weight', 'down_blocks.0.resnets.1.conv1.conv.bias', 'down_blocks.0.resnets.1.time_emb_proj.weight', 'down_blocks.0.resnets.1.time_emb_proj.bias', 'down_blocks.0.resnets.1.norm2.weight', 'down_blocks.0.resnets.1.norm2.bias', 'down_blocks.0.resnets.1.conv2.conv.weight', 'down_blocks.0.resnets.1.conv2.conv.bias', 'down_blocks.0.downsampler.norm1.weight', 'down_blocks.0.downsampler.norm1.bias', 'down_blocks.0.downsampler.conv1.conv.weight', 'down_blocks.0.downsampler.conv1.conv.bias', 'down_blocks.0.downsampler.time_emb_proj.weight', 'down_blocks.0.downsampler.time_emb_proj.bias', 'down_blocks.0.downsampler.norm2.weight', 'down_blocks.0.downsampler.norm2.bias', 'down_blocks.0.downsampler.conv2.conv.weight', 'down_blocks.0.downsampler.conv2.conv.bias', 'down_blocks.1.attentions.0.norm.weight', 'down_blocks.1.attentions.0.norm.bias', 'down_blocks.1.attentions.0.proj_in.conv.weight', 'down_blocks.1.attentions.0.proj_in.conv.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.ff.linear1.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.ff.linear1.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.ff.linear2.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.ff.linear2.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'down_blocks.1.attentions.0.proj_out.conv.weight', 'down_blocks.1.attentions.0.proj_out.conv.bias', 'down_blocks.1.attentions.1.norm.weight', 'down_blocks.1.attentions.1.norm.bias', 'down_blocks.1.attentions.1.proj_in.conv.weight', 'down_blocks.1.attentions.1.proj_in.conv.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.ff.linear1.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.ff.linear1.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.ff.linear2.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.ff.linear2.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'down_blocks.1.attentions.1.proj_out.conv.weight', 'down_blocks.1.attentions.1.proj_out.conv.bias', 'down_blocks.1.resnets.0.norm1.weight', 'down_blocks.1.resnets.0.norm1.bias', 'down_blocks.1.resnets.0.conv1.conv.weight', 'down_blocks.1.resnets.0.conv1.conv.bias', 'down_blocks.1.resnets.0.time_emb_proj.weight', 'down_blocks.1.resnets.0.time_emb_proj.bias', 'down_blocks.1.resnets.0.norm2.weight', 'down_blocks.1.resnets.0.norm2.bias', 'down_blocks.1.resnets.0.conv2.conv.weight', 'down_blocks.1.resnets.0.conv2.conv.bias', 'down_blocks.1.resnets.0.skip_connection.conv.weight', 'down_blocks.1.resnets.0.skip_connection.conv.bias', 'down_blocks.1.resnets.1.norm1.weight', 'down_blocks.1.resnets.1.norm1.bias', 'down_blocks.1.resnets.1.conv1.conv.weight', 'down_blocks.1.resnets.1.conv1.conv.bias', 'down_blocks.1.resnets.1.time_emb_proj.weight', 'down_blocks.1.resnets.1.time_emb_proj.bias', 'down_blocks.1.resnets.1.norm2.weight', 'down_blocks.1.resnets.1.norm2.bias', 'down_blocks.1.resnets.1.conv2.conv.weight', 'down_blocks.1.resnets.1.conv2.conv.bias', 'down_blocks.1.downsampler.norm1.weight', 'down_blocks.1.downsampler.norm1.bias', 'down_blocks.1.downsampler.conv1.conv.weight', 'down_blocks.1.downsampler.conv1.conv.bias', 'down_blocks.1.downsampler.time_emb_proj.weight', 'down_blocks.1.downsampler.time_emb_proj.bias', 'down_blocks.1.downsampler.norm2.weight', 'down_blocks.1.downsampler.norm2.bias', 'down_blocks.1.downsampler.conv2.conv.weight', 'down_blocks.1.downsampler.conv2.conv.bias', 'down_blocks.2.attentions.0.norm.weight', 'down_blocks.2.attentions.0.norm.bias', 'down_blocks.2.attentions.0.proj_in.conv.weight', 'down_blocks.2.attentions.0.proj_in.conv.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.ff.linear1.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.ff.linear1.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.ff.linear2.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.ff.linear2.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'down_blocks.2.attentions.0.proj_out.conv.weight', 'down_blocks.2.attentions.0.proj_out.conv.bias', 'down_blocks.2.attentions.1.norm.weight', 'down_blocks.2.attentions.1.norm.bias', 'down_blocks.2.attentions.1.proj_in.conv.weight', 'down_blocks.2.attentions.1.proj_in.conv.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.ff.linear1.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.ff.linear1.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.ff.linear2.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.ff.linear2.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'down_blocks.2.attentions.1.proj_out.conv.weight', 'down_blocks.2.attentions.1.proj_out.conv.bias', 'down_blocks.2.resnets.0.norm1.weight', 'down_blocks.2.resnets.0.norm1.bias', 'down_blocks.2.resnets.0.conv1.conv.weight', 'down_blocks.2.resnets.0.conv1.conv.bias', 'down_blocks.2.resnets.0.time_emb_proj.weight', 'down_blocks.2.resnets.0.time_emb_proj.bias', 'down_blocks.2.resnets.0.norm2.weight', 'down_blocks.2.resnets.0.norm2.bias', 'down_blocks.2.resnets.0.conv2.conv.weight', 'down_blocks.2.resnets.0.conv2.conv.bias', 'down_blocks.2.resnets.0.skip_connection.conv.weight', 'down_blocks.2.resnets.0.skip_connection.conv.bias', 'down_blocks.2.resnets.1.norm1.weight', 'down_blocks.2.resnets.1.norm1.bias', 'down_blocks.2.resnets.1.conv1.conv.weight', 'down_blocks.2.resnets.1.conv1.conv.bias', 'down_blocks.2.resnets.1.time_emb_proj.weight', 'down_blocks.2.resnets.1.time_emb_proj.bias', 'down_blocks.2.resnets.1.norm2.weight', 'down_blocks.2.resnets.1.norm2.bias', 'down_blocks.2.resnets.1.conv2.conv.weight', 'down_blocks.2.resnets.1.conv2.conv.bias', 'middle_block.resnet_1.norm1.weight', 'middle_block.resnet_1.norm1.bias', 'middle_block.resnet_1.conv1.conv.weight', 'middle_block.resnet_1.conv1.conv.bias', 'middle_block.resnet_1.time_emb_proj.weight', 'middle_block.resnet_1.time_emb_proj.bias', 'middle_block.resnet_1.norm2.weight', 'middle_block.resnet_1.norm2.bias', 'middle_block.resnet_1.conv2.conv.weight', 'middle_block.resnet_1.conv2.conv.bias', 'middle_block.attention.norm.weight', 'middle_block.attention.norm.bias', 'middle_block.attention.proj_in.conv.weight', 'middle_block.attention.proj_in.conv.bias', 'middle_block.attention.transformer_blocks.0.attn1.to_q.weight', 'middle_block.attention.transformer_blocks.0.attn1.to_k.weight', 'middle_block.attention.transformer_blocks.0.attn1.to_v.weight', 'middle_block.attention.transformer_blocks.0.attn1.to_out.0.weight', 'middle_block.attention.transformer_blocks.0.attn1.to_out.0.bias', 'middle_block.attention.transformer_blocks.0.ff.linear1.weight', 'middle_block.attention.transformer_blocks.0.ff.linear1.bias', 'middle_block.attention.transformer_blocks.0.ff.linear2.weight', 'middle_block.attention.transformer_blocks.0.ff.linear2.bias', 'middle_block.attention.transformer_blocks.0.attn2.to_q.weight', 'middle_block.attention.transformer_blocks.0.attn2.to_k.weight', 'middle_block.attention.transformer_blocks.0.attn2.to_v.weight', 'middle_block.attention.transformer_blocks.0.attn2.to_out.0.weight', 'middle_block.attention.transformer_blocks.0.attn2.to_out.0.bias', 'middle_block.attention.transformer_blocks.0.norm1.weight', 'middle_block.attention.transformer_blocks.0.norm1.bias', 'middle_block.attention.transformer_blocks.0.norm2.weight', 'middle_block.attention.transformer_blocks.0.norm2.bias', 'middle_block.attention.transformer_blocks.0.norm3.weight', 'middle_block.attention.transformer_blocks.0.norm3.bias', 'middle_block.attention.proj_out.conv.weight', 'middle_block.attention.proj_out.conv.bias', 'middle_block.resnet_2.norm1.weight', 'middle_block.resnet_2.norm1.bias', 'middle_block.resnet_2.conv1.conv.weight', 'middle_block.resnet_2.conv1.conv.bias', 'middle_block.resnet_2.time_emb_proj.weight', 'middle_block.resnet_2.time_emb_proj.bias', 'middle_block.resnet_2.norm2.weight', 'middle_block.resnet_2.norm2.bias', 'middle_block.resnet_2.conv2.conv.weight', 'middle_block.resnet_2.conv2.conv.bias', 'up_blocks.0.attentions.0.norm.weight', 'up_blocks.0.attentions.0.norm.bias', 'up_blocks.0.attentions.0.proj_in.conv.weight', 'up_blocks.0.attentions.0.proj_in.conv.bias', 'up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.0.attentions.0.transformer_blocks.0.ff.linear1.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.ff.linear1.bias', 'up_blocks.0.attentions.0.transformer_blocks.0.ff.linear2.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.ff.linear2.bias', 'up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias', 'up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias', 'up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias', 'up_blocks.0.attentions.0.proj_out.conv.weight', 'up_blocks.0.attentions.0.proj_out.conv.bias', 'up_blocks.0.attentions.1.norm.weight', 'up_blocks.0.attentions.1.norm.bias', 'up_blocks.0.attentions.1.proj_in.conv.weight', 'up_blocks.0.attentions.1.proj_in.conv.bias', 'up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.0.attentions.1.transformer_blocks.0.ff.linear1.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.ff.linear1.bias', 'up_blocks.0.attentions.1.transformer_blocks.0.ff.linear2.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.ff.linear2.bias', 'up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias', 'up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias', 'up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias', 'up_blocks.0.attentions.1.proj_out.conv.weight', 'up_blocks.0.attentions.1.proj_out.conv.bias', 'up_blocks.0.attentions.2.norm.weight', 'up_blocks.0.attentions.2.norm.bias', 'up_blocks.0.attentions.2.proj_in.conv.weight', 'up_blocks.0.attentions.2.proj_in.conv.bias', 'up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.0.attentions.2.transformer_blocks.0.ff.linear1.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.ff.linear1.bias', 'up_blocks.0.attentions.2.transformer_blocks.0.ff.linear2.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.ff.linear2.bias', 'up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias', 'up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias', 'up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias', 'up_blocks.0.attentions.2.proj_out.conv.weight', 'up_blocks.0.attentions.2.proj_out.conv.bias', 'up_blocks.0.resnets.0.norm1.weight', 'up_blocks.0.resnets.0.norm1.bias', 'up_blocks.0.resnets.0.conv1.conv.weight', 'up_blocks.0.resnets.0.conv1.conv.bias', 'up_blocks.0.resnets.0.time_emb_proj.weight', 'up_blocks.0.resnets.0.time_emb_proj.bias', 'up_blocks.0.resnets.0.norm2.weight', 'up_blocks.0.resnets.0.norm2.bias', 'up_blocks.0.resnets.0.conv2.conv.weight', 'up_blocks.0.resnets.0.conv2.conv.bias', 'up_blocks.0.resnets.0.skip_connection.conv.weight', 'up_blocks.0.resnets.0.skip_connection.conv.bias', 'up_blocks.0.resnets.1.norm1.weight', 'up_blocks.0.resnets.1.norm1.bias', 'up_blocks.0.resnets.1.conv1.conv.weight', 'up_blocks.0.resnets.1.conv1.conv.bias', 'up_blocks.0.resnets.1.time_emb_proj.weight', 'up_blocks.0.resnets.1.time_emb_proj.bias', 'up_blocks.0.resnets.1.norm2.weight', 'up_blocks.0.resnets.1.norm2.bias', 'up_blocks.0.resnets.1.conv2.conv.weight', 'up_blocks.0.resnets.1.conv2.conv.bias', 'up_blocks.0.resnets.1.skip_connection.conv.weight', 'up_blocks.0.resnets.1.skip_connection.conv.bias', 'up_blocks.0.resnets.2.norm1.weight', 'up_blocks.0.resnets.2.norm1.bias', 'up_blocks.0.resnets.2.conv1.conv.weight', 'up_blocks.0.resnets.2.conv1.conv.bias', 'up_blocks.0.resnets.2.time_emb_proj.weight', 'up_blocks.0.resnets.2.time_emb_proj.bias', 'up_blocks.0.resnets.2.norm2.weight', 'up_blocks.0.resnets.2.norm2.bias', 'up_blocks.0.resnets.2.conv2.conv.weight', 'up_blocks.0.resnets.2.conv2.conv.bias', 'up_blocks.0.resnets.2.skip_connection.conv.weight', 'up_blocks.0.resnets.2.skip_connection.conv.bias', 'up_blocks.0.upsampler.norm1.weight', 'up_blocks.0.upsampler.norm1.bias', 'up_blocks.0.upsampler.conv1.conv.weight', 'up_blocks.0.upsampler.conv1.conv.bias', 'up_blocks.0.upsampler.time_emb_proj.weight', 'up_blocks.0.upsampler.time_emb_proj.bias', 'up_blocks.0.upsampler.norm2.weight', 'up_blocks.0.upsampler.norm2.bias', 'up_blocks.0.upsampler.conv2.conv.weight', 'up_blocks.0.upsampler.conv2.conv.bias', 'up_blocks.1.attentions.0.norm.weight', 'up_blocks.1.attentions.0.norm.bias', 'up_blocks.1.attentions.0.proj_in.conv.weight', 'up_blocks.1.attentions.0.proj_in.conv.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.ff.linear1.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.ff.linear1.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.ff.linear2.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.ff.linear2.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'up_blocks.1.attentions.0.proj_out.conv.weight', 'up_blocks.1.attentions.0.proj_out.conv.bias', 'up_blocks.1.attentions.1.norm.weight', 'up_blocks.1.attentions.1.norm.bias', 'up_blocks.1.attentions.1.proj_in.conv.weight', 'up_blocks.1.attentions.1.proj_in.conv.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.ff.linear1.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.ff.linear1.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.ff.linear2.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.ff.linear2.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'up_blocks.1.attentions.1.proj_out.conv.weight', 'up_blocks.1.attentions.1.proj_out.conv.bias', 'up_blocks.1.attentions.2.norm.weight', 'up_blocks.1.attentions.2.norm.bias', 'up_blocks.1.attentions.2.proj_in.conv.weight', 'up_blocks.1.attentions.2.proj_in.conv.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.ff.linear1.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.ff.linear1.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.ff.linear2.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.ff.linear2.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias', 'up_blocks.1.attentions.2.proj_out.conv.weight', 'up_blocks.1.attentions.2.proj_out.conv.bias', 'up_blocks.1.resnets.0.norm1.weight', 'up_blocks.1.resnets.0.norm1.bias', 'up_blocks.1.resnets.0.conv1.conv.weight', 'up_blocks.1.resnets.0.conv1.conv.bias', 'up_blocks.1.resnets.0.time_emb_proj.weight', 'up_blocks.1.resnets.0.time_emb_proj.bias', 'up_blocks.1.resnets.0.norm2.weight', 'up_blocks.1.resnets.0.norm2.bias', 'up_blocks.1.resnets.0.conv2.conv.weight', 'up_blocks.1.resnets.0.conv2.conv.bias', 'up_blocks.1.resnets.0.skip_connection.conv.weight', 'up_blocks.1.resnets.0.skip_connection.conv.bias', 'up_blocks.1.resnets.1.norm1.weight', 'up_blocks.1.resnets.1.norm1.bias', 'up_blocks.1.resnets.1.conv1.conv.weight', 'up_blocks.1.resnets.1.conv1.conv.bias', 'up_blocks.1.resnets.1.time_emb_proj.weight', 'up_blocks.1.resnets.1.time_emb_proj.bias', 'up_blocks.1.resnets.1.norm2.weight', 'up_blocks.1.resnets.1.norm2.bias', 'up_blocks.1.resnets.1.conv2.conv.weight', 'up_blocks.1.resnets.1.conv2.conv.bias', 'up_blocks.1.resnets.1.skip_connection.conv.weight', 'up_blocks.1.resnets.1.skip_connection.conv.bias', 'up_blocks.1.resnets.2.norm1.weight', 'up_blocks.1.resnets.2.norm1.bias', 'up_blocks.1.resnets.2.conv1.conv.weight', 'up_blocks.1.resnets.2.conv1.conv.bias', 'up_blocks.1.resnets.2.time_emb_proj.weight', 'up_blocks.1.resnets.2.time_emb_proj.bias', 'up_blocks.1.resnets.2.norm2.weight', 'up_blocks.1.resnets.2.norm2.bias', 'up_blocks.1.resnets.2.conv2.conv.weight', 'up_blocks.1.resnets.2.conv2.conv.bias', 'up_blocks.1.resnets.2.skip_connection.conv.weight', 'up_blocks.1.resnets.2.skip_connection.conv.bias', 'up_blocks.1.upsampler.norm1.weight', 'up_blocks.1.upsampler.norm1.bias', 'up_blocks.1.upsampler.conv1.conv.weight', 'up_blocks.1.upsampler.conv1.conv.bias', 'up_blocks.1.upsampler.time_emb_proj.weight', 'up_blocks.1.upsampler.time_emb_proj.bias', 'up_blocks.1.upsampler.norm2.weight', 'up_blocks.1.upsampler.norm2.bias', 'up_blocks.1.upsampler.conv2.conv.weight', 'up_blocks.1.upsampler.conv2.conv.bias', 'up_blocks.2.resnets.0.norm1.weight', 'up_blocks.2.resnets.0.norm1.bias', 'up_blocks.2.resnets.0.conv1.conv.weight', 'up_blocks.2.resnets.0.conv1.conv.bias', 'up_blocks.2.resnets.0.time_emb_proj.weight', 'up_blocks.2.resnets.0.time_emb_proj.bias', 'up_blocks.2.resnets.0.norm2.weight', 'up_blocks.2.resnets.0.norm2.bias', 'up_blocks.2.resnets.0.conv2.conv.weight', 'up_blocks.2.resnets.0.conv2.conv.bias', 'up_blocks.2.resnets.0.skip_connection.conv.weight', 'up_blocks.2.resnets.0.skip_connection.conv.bias', 'up_blocks.2.resnets.1.norm1.weight', 'up_blocks.2.resnets.1.norm1.bias', 'up_blocks.2.resnets.1.conv1.conv.weight', 'up_blocks.2.resnets.1.conv1.conv.bias', 'up_blocks.2.resnets.1.time_emb_proj.weight', 'up_blocks.2.resnets.1.time_emb_proj.bias', 'up_blocks.2.resnets.1.norm2.weight', 'up_blocks.2.resnets.1.norm2.bias', 'up_blocks.2.resnets.1.conv2.conv.weight', 'up_blocks.2.resnets.1.conv2.conv.bias', 'up_blocks.2.resnets.1.skip_connection.conv.weight', 'up_blocks.2.resnets.1.skip_connection.conv.bias', 'up_blocks.2.resnets.2.norm1.weight', 'up_blocks.2.resnets.2.norm1.bias', 'up_blocks.2.resnets.2.conv1.conv.weight', 'up_blocks.2.resnets.2.conv1.conv.bias', 'up_blocks.2.resnets.2.time_emb_proj.weight', 'up_blocks.2.resnets.2.time_emb_proj.bias', 'up_blocks.2.resnets.2.norm2.weight', 'up_blocks.2.resnets.2.norm2.bias', 'up_blocks.2.resnets.2.conv2.conv.weight', 'up_blocks.2.resnets.2.conv2.conv.bias', 'up_blocks.2.resnets.2.skip_connection.conv.weight', 'up_blocks.2.resnets.2.skip_connection.conv.bias', 'out.0.weight', 'out.0.bias', 'out.2.conv.weight', 'out.2.conv.bias'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(os.path.join('/home/huutien/sources/GenerativeModels/large_files', \"diffusion_model.pth\"), map_location=device)\n",
    "ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdfe7be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 7, 3, 3, 3]), torch.Size([256]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['conv_in.conv.weight'].shape, ckpt['conv_in.conv.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a92ebf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 3, 3, 3, 3]), torch.Size([256]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_model.state_dict()['conv_in.conv.weight'].shape , diffusion_model.state_dict()['conv_in.conv.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc84ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_adapter = StateDictAdapter()\n",
    "state_dict = state_dict_adapter(\n",
    "    model_state_dict=diffusion_model.state_dict(),\n",
    "    checkpoint_state_dict=ckpt,\n",
    "    regex_keys=[\n",
    "        r\"conv_in.conv.weight\",\n",
    "    ],\n",
    "    strategy=\"zeros\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed97d67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['conv_in.conv.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb236a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['conv_in.conv.weight'][:,:3,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d73d51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20736)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(state_dict['conv_in.conv.weight'] == ckpt['conv_in.conv.weight'][:,:3,:,:,:]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af88e719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ff60eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffusionModelUNet(\n",
       "  (conv_in): Convolution(\n",
       "    (conv): Conv3d(3, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  )\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): DownBlock(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (time_emb_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (downsampler): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (downsample): Downsample(\n",
       "          (op): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (time_emb_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnDownBlock(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (proj_in): Convolution(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): MLPBlock(\n",
       "                (linear1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "                (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (fn): GEGLU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=4, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=4, out_features=512, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Convolution(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (time_emb_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (skip_connection): Convolution(\n",
       "            (conv): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (time_emb_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (downsampler): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (downsample): Downsample(\n",
       "          (op): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (time_emb_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnDownBlock(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "          (proj_in): Convolution(\n",
       "            (conv): Conv3d(768, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): MLPBlock(\n",
       "                (linear1): Linear(in_features=768, out_features=6144, bias=True)\n",
       "                (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (fn): GEGLU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (to_k): Linear(in_features=4, out_features=768, bias=False)\n",
       "                (to_v): Linear(in_features=4, out_features=768, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Convolution(\n",
       "            (conv): Conv3d(768, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(512, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (time_emb_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "          (norm2): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (skip_connection): Convolution(\n",
       "            (conv): Conv3d(512, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (time_emb_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "          (norm2): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle_block): CrossAttnMidBlock(\n",
       "    (resnet_1): ResnetBlock(\n",
       "      (norm1): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "      (nonlinearity): SiLU()\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      )\n",
       "      (time_emb_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "      (norm2): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (attention): SpatialTransformer(\n",
       "      (norm): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "      (proj_in): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (attn1): CrossAttention(\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (ff): MLPBlock(\n",
       "            (linear1): Linear(in_features=768, out_features=6144, bias=True)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (fn): GEGLU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (attn2): CrossAttention(\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_k): Linear(in_features=4, out_features=768, bias=False)\n",
       "            (to_v): Linear(in_features=4, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "    )\n",
       "    (resnet_2): ResnetBlock(\n",
       "      (norm1): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "      (nonlinearity): SiLU()\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      )\n",
       "      (time_emb_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "      (norm2): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): CrossAttnUpBlock(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "          (proj_in): Convolution(\n",
       "            (conv): Conv3d(768, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): MLPBlock(\n",
       "                (linear1): Linear(in_features=768, out_features=6144, bias=True)\n",
       "                (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (fn): GEGLU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (to_k): Linear(in_features=4, out_features=768, bias=False)\n",
       "                (to_v): Linear(in_features=4, out_features=768, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Convolution(\n",
       "            (conv): Conv3d(768, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 1536, eps=1e-06, affine=True)\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(1536, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (time_emb_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "          (norm2): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (skip_connection): Convolution(\n",
       "            (conv): Conv3d(1536, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(1280, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (time_emb_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "          (norm2): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (skip_connection): Convolution(\n",
       "            (conv): Conv3d(1280, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (upsampler): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (upsample): Upsample()\n",
       "        (time_emb_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "        (norm2): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnUpBlock(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (proj_in): Convolution(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): MLPBlock(\n",
       "                (linear1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "                (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (fn): GEGLU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=4, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=4, out_features=512, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Convolution(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(1280, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (time_emb_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (skip_connection): Convolution(\n",
       "            (conv): Conv3d(1280, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(1024, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (time_emb_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (skip_connection): Convolution(\n",
       "            (conv): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(768, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (time_emb_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (skip_connection): Convolution(\n",
       "            (conv): Conv3d(768, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (upsampler): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (upsample): Upsample()\n",
       "        (time_emb_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock(\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 768, eps=1e-06, affine=True)\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(768, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (time_emb_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (skip_connection): Convolution(\n",
       "            (conv): Conv3d(768, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (time_emb_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          )\n",
       "          (skip_connection): Convolution(\n",
       "            (conv): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "    (1): SiLU()\n",
       "    (2): Convolution(\n",
       "      (conv): Conv3d(256, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_model.load_state_dict(state_dict, strict=True)\n",
    "diffusion_model.to(device)\n",
    "\n",
    "diffusion_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6450122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditioning = torch.tensor([[0., 0, 0, 0]]).to( device).unsqueeze(1)\n",
    "conditioning = torch.zeros(2,1, 4).to(device)\n",
    "# use yaml config \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90d1867e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditioning.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9750c6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 20, 20, 12])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "latent = latent.to(device)\n",
    "timesteps = torch.zeros(2, device=device)\n",
    "prediction = diffusion_model(latent, timesteps, context=conditioning)\n",
    "prediction.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d947ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_timestep_embedding(timesteps: torch.Tensor, embedding_dim: int, max_period: int = 10000) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings following the implementation in Ho et al. \"Denoising Diffusion Probabilistic\n",
    "    Models\" https://arxiv.org/abs/2006.11239.\n",
    "\n",
    "    Args:\n",
    "        timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "        embedding_dim: the dimension of the output.\n",
    "        max_period: controls the minimum frequency of the embeddings.\n",
    "    \"\"\"\n",
    "    if timesteps.ndim != 1:\n",
    "        raise ValueError(\"Timesteps should be a 1d-array\")\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32, device=timesteps.device)\n",
    "    freqs = torch.exp(exponent / half_dim)\n",
    "\n",
    "    args = timesteps[:, None].float() * freqs[None, :]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "\n",
    "    # zero pad\n",
    "    if embedding_dim % 2 == 1:\n",
    "        embedding = torch.nn.functional.pad(embedding, (0, 1, 0, 0))\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "242c041b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m t_emb\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mlatent\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m\n\u001b[0;32m----> 9\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/anaconda3/envs/m3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/anaconda3/envs/m3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/anaconda3/envs/m3d/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/home/anaconda3/envs/m3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/anaconda3/envs/m3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/anaconda3/envs/m3d/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "t_emb = get_timestep_embedding(timesteps, diffusion_model.block_out_channels[0])\n",
    "\n",
    "# timesteps does not contain any weights and will always return f32 tensors\n",
    "# but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "# there might be better ways to encapsulate this.\n",
    "t_emb = t_emb.to(dtype=latent.dtype)\n",
    "print\n",
    "\n",
    "emb = diffusion_model.time_embed(t_emb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1c9fc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_emb.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02ddaad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_model.time_embed[0].weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "336ac2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_model.conv_in.conv.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f6cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
